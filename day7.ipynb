{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYUaN4RQQIoDU09cVn0cXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessica-hoffman/transformer_practice/blob/main/day7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kxeX2wYIhLpd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple model: small feedforward net\n",
        "class TinyModel(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=256, output_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "BYaZoyIZhSp3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('using device:', device)\n",
        "if device.type == 'cuda':\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTJICbDxhUpZ",
        "outputId": "547c0d01-bbfb-4df8-acb5-46800371a618"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TinyModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "def profile_batch(batch_size, seq_len=512, num_classes=10):\n",
        "    print(f'\\n=== Batch size {batch_size} ===')\n",
        "\n",
        "    # generate random input and labels\n",
        "    x = torch.randn(batch_size, seq_len, device=device)\n",
        "    y = torch.randint(0, num_classes, (batch_size,), device=device)\n",
        "\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # record memory before forward\n",
        "    torch.cuda.empty_cache()\n",
        "    if device.type == 'cuda':\n",
        "        mem_before = torch.cuda.memory_allocated(device)/1024**2\n",
        "    else:\n",
        "        mem_before = 0.0\n",
        "\n",
        "    # forward pass timing\n",
        "    start = time.time()\n",
        "    outputs = model(x)\n",
        "    loss = criterion(outputs, y)\n",
        "    fwd_time = time.time() - start\n",
        "\n",
        "    # backward pass timing\n",
        "    start = time.time()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    bwd_time = time.time() - start\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        mem_after = torch.cuda.memory_allocated(device)/1024**2\n",
        "    else:\n",
        "        mem_after = 0.0\n",
        "\n",
        "    print(f'Forward time: {fwd_time:.4f}s, Backward time: {bwd_time:.4f}s')\n",
        "    print(f'Memory before: {mem_before:.2f} MB, after: {mem_after:.2f} MB')\n",
        "\n",
        "# try different batch sizes\n",
        "for bs in [8,32,128,512]:\n",
        "    profile_batch(bs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyFVsjBTi9lK",
        "outputId": "090ef458-de03-46ee-d8ff-075cba6b7cec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Batch size 8 ===\n",
            "Forward time: 0.3116s, Backward time: 0.3331s\n",
            "Memory before: 0.53 MB, after: 18.31 MB\n",
            "\n",
            "=== Batch size 32 ===\n",
            "Forward time: 0.0053s, Backward time: 0.0013s\n",
            "Memory before: 17.85 MB, after: 18.36 MB\n",
            "\n",
            "=== Batch size 128 ===\n",
            "Forward time: 0.0004s, Backward time: 0.0012s\n",
            "Memory before: 18.03 MB, after: 18.55 MB\n",
            "\n",
            "=== Batch size 512 ===\n",
            "Forward time: 0.0006s, Backward time: 0.0011s\n",
            "Memory before: 18.79 MB, after: 19.32 MB\n"
          ]
        }
      ]
    }
  ]
}