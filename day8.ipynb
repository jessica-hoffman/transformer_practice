{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPeYopbPqyf6QLq5SN1tgRY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessica-hoffman/transformer_practice/blob/main/day8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Day 8 ‚Äì Debugging & Problem Solving (2 hrs)\n",
        "\n",
        "**Goal:** Be able to fix errors quickly.\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Reading (~30 min)\n",
        "\n",
        "### 1. Common PyTorch Errors\n",
        "- **Device mismatch (`RuntimeError: Expected all tensors to be on the same device`)**\n",
        "  - Happens if model is on GPU but data is still on CPU (or vice versa).\n",
        "  - Fix: `.to(device)` on both model and tensors.\n",
        "\n",
        "- **GradScaler warnings (AMP training)**\n",
        "  - AMP = Automatic Mixed Precision.\n",
        "  - Errors like `GradScaler has been disabled` ‚Üí usually because loss became NaN.\n",
        "  - Fix: reduce learning rate, check for exploding gradients, clip gradients.\n",
        "\n",
        "- **Shape mismatch**\n",
        "  - Example: logits shape `[batch_size, vocab_size]` vs labels shape `[batch_size, seq_len]`.\n",
        "  - Fix: make sure labels align with model output (e.g. shift labels in language modeling).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Hugging Face Specific Errors\n",
        "- **`KeyError: 'input_ids'`**\n",
        "  - Hugging Face models expect inputs in dicts with keys: `input_ids`, `attention_mask`, etc.\n",
        "  - Fix: make sure your tokenizer output is passed as `**batch`.\n",
        "\n",
        "- **`ValueError: Tokenizer does not have a pad_token`**\n",
        "  - Needed for batching sequences of different lengths.\n",
        "  - Fix:\n",
        "    ```python\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    ```\n",
        "\n",
        "- **`RuntimeError: CUDA out of memory`**\n",
        "  - Fixes: reduce batch size, use gradient accumulation, switch to smaller model, use `.half()`.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† Hands-on (~1.5 hr)\n",
        "\n",
        "We‚Äôll **intentionally introduce errors** and practice debugging.\n",
        "\n",
        "### 1. Device mismatch\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C0c8sPuiBbmc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqMO55CGBWN_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = nn.Linear(10, 2).to(device)\n",
        "x = torch.randn(4, 10)   # Oops! Still on CPU\n",
        "\n",
        "# ‚ùå Will error:\n",
        "out = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix:"
      ],
      "metadata": {
        "id": "LcW2fToLBunL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.to(device)\n",
        "out = model(x)"
      ],
      "metadata": {
        "id": "WWIebufZBv3I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. KeyError: 'input_ids'"
      ],
      "metadata": {
        "id": "mKPaIyODB6uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "text = ['hello world']\n",
        "batch = tokenizer(text, padding=True, return_tensors='pt')\n",
        "\n",
        "# ‚ùå Mistakenly try to access wrong key\n",
        "print(batch[\"ids\"])"
      ],
      "metadata": {
        "id": "bNczQC42B8BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix:"
      ],
      "metadata": {
        "id": "3pP-PFRCB8iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP-4Fnk5CJ4p",
        "outputId": "60db7451-ca36-4853-df15-b7462cbbac36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 101, 7592, 2088,  102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tokenizer pad_token issue"
      ],
      "metadata": {
        "id": "W8QDpEulCM26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tok = AutoTokenizer.from_pretrained('gpt2')\n",
        "tok.pad_token = None\n",
        "\n",
        "# ‚ùå This will error because GPT-2 has no pad_token\n",
        "batch = tok([\"hi\", \"there\"], padding=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "XlnnkouVCO2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix:"
      ],
      "metadata": {
        "id": "Pn7pKrYcCTxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok.pad_token = tok.eos_token\n",
        "batch = tok(['hi', 'there'], padding=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "ceceuUFtCVPx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Shape mismatch"
      ],
      "metadata": {
        "id": "15aJokLXCXM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.randn(4,5)   # (batch, classes)\n",
        "labels = torch.randint(0, 5, (4,3)) # (batch, seq_len) ‚ùå mismatch\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss = loss_fn(logits, labels)"
      ],
      "metadata": {
        "id": "Bkwfs7vsCYv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix:"
      ],
      "metadata": {
        "id": "LbVf_4tjCb80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.randint(0, 5, (4,)) # match (batch,)\n",
        "loss = loss_fn(logits, labels)"
      ],
      "metadata": {
        "id": "z7HQJAxjCdIX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. AMP / GradScaler"
      ],
      "metadata": {
        "id": "-epwXgv_CrO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "for i in range(100):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        out = model(torch.randn(32, 10).to(device))\n",
        "        loss = out.mean() * 1e6  #  # ‚ùå artificial overflow\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optim)\n",
        "    scaler.update()"
      ],
      "metadata": {
        "id": "zP9MgIYfCszA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fixes:\n",
        "\n",
        "Lower learning rate.\n",
        "\n",
        "Clip gradients.\n",
        "\n",
        "Avoid extreme scaling."
      ],
      "metadata": {
        "id": "UvKgberRDB1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=1e-5)   # lower learning rate\n",
        "\n",
        "for i in range(100):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        out = model(torch.randn(32, 10).to(device))\n",
        "        loss = out.mean()                       # remove the high scaling coefficient that causes overflow\n",
        "    scaler.scale(loss).backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)        # gradient clipping right before step - this keeps gradient from exploding\n",
        "    scaler.step(optim)\n",
        "    scaler.update()\n"
      ],
      "metadata": {
        "id": "euA13zyADCsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}